{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n",
    "For each element in the input sequence, each layer computes the following function\n",
    "\\begin{array}{ll}\n",
    "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
    "h_t = o_t \\tanh(c_t)\n",
    "\\end{array}\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell\n",
    "state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$\n",
    "is the hidden state of the previous layer at time $t-1$ or the initial hidden\n",
    "state at time $0$, and $i_t$, $f_t$, $g_t$,\n",
    "$o_t$ are the input, forget, cell, and output gates, respectively.\n",
    "$\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Args:\n",
    "\n",
    "input_size: The number of expected features in the input $x$\n",
    "\n",
    "hidden_size: The number of features in the hidden state $h$\n",
    "\n",
    "num_layers: Number of recurrent layers. E.g., setting $num_layers=2$ would mean stacking two LSTMs together to form a $stacked LSTM$, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "\n",
    "bias: If $False$, then the layer does not use bias weights $b_ih$ and $b_hh$. Default: $True$\n",
    "\n",
    "batch_first: If $True$, then the input and output tensors are provided as (batch, seq, feature). Default: $False$\n",
    "\n",
    "dropout: If non-zero, introduces a $Dropout$ layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to :attr:$dropout$. Default: 0\n",
    "\n",
    "bidirectional: If $True$, becomes a bidirectional LSTM. Default: $False$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: input, ($h_0$, $c_0$)\n",
    "    - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features of the input sequence.\n",
    "      The input can also be a packed variable length sequence.\n",
    "      See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
    "      :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
    "    - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "      containing the initial hidden state for each element in the batch.\n",
    "    - **c_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "      containing the initial cell state for each element in the batch.\n",
    "\n",
    "      If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero.\n",
    "\n",
    "\n",
    "Outputs: output, (h_n, c_n)\n",
    "    - **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor\n",
    "      containing the output features `(h_t)` from the last layer of the LSTM,\n",
    "      for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
    "      given as the input, the output will also be a packed sequence.\n",
    "\n",
    "      For the unpacked case, the directions can be separated\n",
    "      using ``output.view(seq_len, batch, num_directions, hidden_size)``,\n",
    "      with forward and backward being direction `0` and `1` respectively.\n",
    "      Similarly, the directions can be separated in the packed case.\n",
    "    - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "      containing the hidden state for `t = seq_len`.\n",
    "\n",
    "      Like *output*, the layers can be separated using\n",
    "      ``h_n.view(num_layers, num_directions, batch, hidden_size)`` and similarly for *c_n*.\n",
    "    - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
    "      containing the cell state for `t = seq_len`\n",
    "\n",
    "Attributes:\n",
    "\n",
    "    weight_ih_l[k] : the learnable input-hidden weights of the :math:$\\text{k}^{th}$ layer\n",
    "        $(W_ii|W_if|W_ig|W_io)$, of shape $(4*hidden_size x input_size)$\n",
    "        \n",
    "    weight_hh_l[k] : the learnable hidden-hidden weights of the :math:$\\text{k}^{th}$ layer\n",
    "        $(W_hi|W_hf|W_hg|W_ho)$, of shape $(4*hidden_size x hidden_size)$\n",
    "        \n",
    "    bias_ih_l[k] : the learnable input-hidden bias of the :math:$\\text{k}^{th}$ layer\n",
    "        $(b_ii|b_if|b_ig|b_io)$, of shape $(4*hidden_size)$\n",
    "        \n",
    "    bias_hh_l[k] : the learnable hidden-hidden bias of the :math:$\\text{k}^{th}$ layer\n",
    "        $(b_hi|b_hf|b_hg|b_ho)$, of shape $(4*hidden_size)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(input_size=1, hidden_size=2, num_layers=2, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.8739]],\n",
      "\n",
      "        [[ 0.4936]]])\n",
      "tensor([[[-0.5848, -1.9906]],\n",
      "\n",
      "        [[-1.4846,  1.2139]],\n",
      "\n",
      "        [[-1.3665,  1.0668]],\n",
      "\n",
      "        [[ 0.1686, -0.9288]]])\n",
      "tensor([[[ 0.6920, -0.0524]],\n",
      "\n",
      "        [[-1.0819, -1.1228]],\n",
      "\n",
      "        [[ 0.1523,  1.1516]],\n",
      "\n",
      "        [[-0.2242, -0.2762]]])\n"
     ]
    }
   ],
   "source": [
    "# seq_len = 2, batch=1, input_size=1\n",
    "i = torch.randn(2, 1, 1)\n",
    "# num_layer * bi = 4, batch=1, hidden=2\n",
    "h0 = torch.randn(4, 1, 2)\n",
    "# c0 和 h0一样\n",
    "c0 = torch.randn(4, 1, 2)\n",
    "print(i)\n",
    "print(h0)\n",
    "print(c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0426,  0.5477, -0.0405,  0.0072]],\n",
      "\n",
      "        [[-0.0620,  0.4924, -0.0441,  0.0820]]], grad_fn=<CatBackward>)\n",
      "tensor([[[ 0.2159, -0.0716]],\n",
      "\n",
      "        [[-0.0695, -0.0517]],\n",
      "\n",
      "        [[-0.0620,  0.4924]],\n",
      "\n",
      "        [[-0.0405,  0.0072]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[ 0.5696, -0.2519]],\n",
      "\n",
      "        [[-0.2497, -0.5190]],\n",
      "\n",
      "        [[-0.1223,  0.8234]],\n",
      "\n",
      "        [[-0.1302,  0.0169]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, (hn, cn) = rnn(i, (h0, c0))\n",
    "# seq_len = 2, batch = 1, hidden=2 * 2 = 4\n",
    "print(output)\n",
    "# hn 和 cn 保持不变\n",
    "print(hn)\n",
    "print(cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output[0][0:2] 是正向的第一词的输出， output[0][2:4]是正向的第二词的输出（第二层）\n",
    "output[1][0:2] 是正向的第2词的输出， output[0][2:4]是正向的第1词的输出（第二层）\n",
    "\n",
    "output[0][2:4] = hn[3][0]， output[1][0:2] = hn[2][0] 其是第二层正负向的隐层向量，h0h1为第一层。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following\n",
    "function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{ll}\n",
    "r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
    "z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
    "n_t = \\tanh(W_{in} x_t + b_{in} + r_t (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
    "h_t = (1 - z_t) n_t + z_t h_{(t-1)} \\\\\n",
    "\\end{array}\n",
    "\n",
    "where \n",
    "\n",
    "$h_t$ is the hidden state at time $t$, \n",
    "\n",
    "$x_t$ is the inputat time $t$, $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$, \n",
    "\n",
    "$r_t$,$z_t$, $n_t$ are the reset, update, and new gates, respectively.\n",
    "\n",
    "$\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Args:\n",
    "    input_size: The number of expected features in the input `x`\n",
    "    \n",
    "    hidden_size: The number of features in the hidden state `h`\n",
    "    \n",
    "    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
    "        would mean stacking two GRUs together to form a `stacked GRU`,\n",
    "        with the second GRU taking in outputs of the first GRU and\n",
    "        computing the final results. Default: 1\n",
    "        \n",
    "    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
    "        Default: ``True``\n",
    "        \n",
    "    batch_first: If ``True``, then the input and output tensors are provided\n",
    "        as (batch, seq, feature). Default: ``False``\n",
    "        \n",
    "    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
    "        GRU layer except the last layer, with dropout probability equal to\n",
    "        :attr:`dropout`. Default: 0\n",
    "        \n",
    "    bidirectional: If ``True``, becomes a bidirectional GRU. Default: ``False``\n",
    "\n",
    "Inputs: input, h_0\n",
    "    - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n",
    "      of the input sequence. The input can also be a packed variable length\n",
    "      sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n",
    "      for details.\n",
    "    - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "      containing the initial hidden state for each element in the batch.\n",
    "      Defaults to zero if not provided.\n",
    "\n",
    "Outputs: output, h_n\n",
    "    - **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor\n",
    "      containing the output features h_t from the last layer of the GRU,\n",
    "      for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
    "      given as the input, the output will also be a packed sequence.\n",
    "      For the unpacked case, the directions can be separated\n",
    "      using ``output.view(seq_len, batch, num_directions, hidden_size)``,\n",
    "      with forward and backward being direction `0` and `1` respectively.\n",
    "\n",
    "      Similarly, the directions can be separated in the packed case.\n",
    "    - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
    "      containing the hidden state for `t = seq_len`\n",
    "\n",
    "      Like *output*, the layers can be separated using\n",
    "      ``h_n.view(num_layers, num_directions, batch, hidden_size)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size=1, hidden_size=2, num_layers=2, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0798]],\n",
      "\n",
      "        [[-1.4793]]])\n",
      "tensor([[[-0.3442,  1.2287]],\n",
      "\n",
      "        [[-0.2276, -0.4527]],\n",
      "\n",
      "        [[ 1.5438,  0.1433]],\n",
      "\n",
      "        [[-0.0715, -0.4056]]])\n"
     ]
    }
   ],
   "source": [
    "# seq_len = 2, batch=1, input_size=1\n",
    "i = torch.randn(2, 1, 1)\n",
    "# num_layer * bi = 4, batch=1, hidden=2\n",
    "h0 = torch.randn(4, 1, 2)\n",
    "print(i)\n",
    "print(h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0777,  0.4532, -0.3319, -0.5030]],\n",
      "\n",
      "        [[ 0.6944,  0.5773, -0.4130, -0.4684]]], grad_fn=<CatBackward>)\n",
      "tensor([[[-0.7958,  0.2209]],\n",
      "\n",
      "        [[-0.5869, -0.3335]],\n",
      "\n",
      "        [[ 0.6944,  0.5773]],\n",
      "\n",
      "        [[-0.3319, -0.5030]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, hn = gru(i, h0)\n",
    "# seq_len = 2, batch = 1, hidden=2 * 2 = 4\n",
    "print(output)\n",
    "# hn 和 cn 保持不变\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
